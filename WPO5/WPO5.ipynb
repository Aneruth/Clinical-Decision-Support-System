{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f511eff26871492f3fa4ae077230fe51",
     "grade": false,
     "grade_id": "cell-f77284e87f75a760",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Health Information Systems and Decision Support Systems\n",
    "# WPO 5: - CAD Systems (25/03/2022)\n",
    "***\n",
    "*Panagiotis Gonidakis, Jakub Ceranka, Joris Wuts, Jef Vandemeulebrouke*<br>\n",
    "*Department of Electronics and Informatics (ETRO)*<br>\n",
    "*Vrije Universiteit Brussel, Pleinlaan 2, B-1050 Brussels, Belgium*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Insert students name and IDs here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "35e56d1502506c7d95a2179be1975802",
     "grade": false,
     "grade_id": "cell-1180f9385da3b954",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Goal\n",
    "The goal of this practical session is to get an insight into artificial neural networks and convolutional neural networks. Your tasks will involve examining and preprocessing the data, training various neural networks and validating the performance of your system against the ground-truth predictions done manually by an experienced radiologist. Students must send their notebook, the image files representing the necessary graphs and the report in .ipynb and .html format. The grade from this practical session will contribute to your final grade.\n",
    "\n",
    "You are kindly requested to submit the <b> .ipynb </b>, an exported <b>.html</b> version  with all the cells properly executed and a <b>.zip</b> file containing all the <b>images</b> displaying the tensorboard training/validating curves </b>(in case are not displayed in the .ipynb).\n",
    "\n",
    "Please use the Canvas assignment functionality to upload your reports.\n",
    "\n",
    "The deadline of the submission is in <b> 01/04/2022, 23:59 pm. </b>.\n",
    "\n",
    "Questions: [pgonidak@etrovub.be](mailto:pgonidak@etrovub.be) and [jceranka@etrovub.be](mailto:jceranka@etrovub.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5287e4bf438bc9b077ef9f2e4bcec518",
     "grade": false,
     "grade_id": "cell-65dd4b67d3695b43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Libraries\n",
    "During this practical session, the following libraries will be used:\n",
    "\n",
    "* Numpy (np)\n",
    "* Glob\n",
    "* Simple ITK (sitk)\n",
    "* Matplotlib\n",
    "* Sklearn\n",
    "* Tensorflow (tf)\n",
    "* Keras (you can also use pytorch if you are more familiar)\n",
    "\n",
    "To import any external library, you need to import it using the **import** statement followed by the name of the library and the shortcut. You can additionally check for the module version using **version** command. \n",
    "\n",
    "* If you use your own laptop, you will need to install new modules.\n",
    "\n",
    "* These expirements are simplified in order to be run without the need of a powerful GPU. However some training tasks may take 30-40 minutes using a CPU. You can accelerate your expirements if you work on [Google colab](https://colab.research.google.com/) framework where a GPU is offered. Then you need to create a GoogleDrive account and upload all the necessary data (scripts + data). \n",
    "For more information look [here](https://colab.research.google.com/) and [here](https://colab.research.google.com/notebooks/gpu.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "90dddf608397c1410372081b65393f39",
     "grade": false,
     "grade_id": "cell-cfeba9f5a5235a75",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Tensorflow and Keras (or TFLearn)\n",
    "\n",
    "TensorFlow (https://www.tensorflow.org/) is an end-to-end open-source platform for machine learning. Itâ€™s a comprehensive and flexible ecosystem of tools, libraries and other resources that provide workflows with high-level APIs. The framework offers various levels of concepts for you to choose the one you need to build and deploy machine learning models.\n",
    "\n",
    "Keras (https://keras.io/), on the other hand, is a high-level neural networks library that is running on the top of TensorFlow. Using Keras in deep learning allows for easy and fast prototyping as well as running seamlessly on CPU and GPU. This framework is written in Python code which is easy to debug and allows ease for extensibility. Keras offers simple and consistent high-level APIs and follows best practices to reduce the cognitive load for the users. Both frameworks provide high-level APIs for building and training models with ease. Keras is built in Python which makes it way more user-friendly than TensorFlow. \n",
    "\n",
    "In this exercise, <b>we recommend to use Keras</b> which allows to create/train/test neural networks with very few lines of understandable code. \n",
    "\n",
    "Keras (or TFLearn) requires Tensorflow to be installed. Normally, the new versions of keras installs it automatically.\n",
    "\n",
    "To install keras, open the anaconda prompt and then type:\n",
    " ```pip install keras```\n",
    "    \n",
    "If not done automatically:\n",
    " ```pip install tensorflow``` and after ```pip install tflearn```\n",
    "\n",
    "\n",
    "Here are some useful resources (installation + documentation):\n",
    "\n",
    "* https://phoenixnap.com/kb/how-to-install-keras-on-linux <p>\n",
    "* https://www.tensorflow.org/install <p>\n",
    "* https://keras.io/getting_started/ <p>\n",
    "* https://keras.io/api/models/ <p>\n",
    "* https://keras.io/api/models/sequential/ <p>\n",
    "    \n",
    "(*) For the purpose of this exercise, you don't need to use a GPU; tensorflow and keras can be configured to run on a CPU and the most demanding training tasks should not take more than 30min. On the other hand, [colab](https://colab.research.google.com/) offers GPU development environment and the training tasks can really be accelerated.\n",
    " \n",
    "(*) We recommend using tensorflow 2. It is advised to verify which version it is being used before looking for any documentation as the APIs differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f6b761e553767615fad950fc7d54345",
     "grade": false,
     "grade_id": "cell-e04efc9902fe123b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lung Nodule Analysis \n",
    "Lung cancer is the leading cause of cancer-related deaths worldwide. Screening high risk individuals for lung cancer with low-dose CT scans is now being implemented in the United States and other countries are expected to follow soon. In CT lung cancer screening, millions of CT scans will have to be analyzed, which is an enormous burden for radiologists. Therefore, there is a lot of interest in development of computer algorithms to optimize cancer screening.\n",
    "\n",
    "<img src=\"images/luna16_image.png\">\n",
    "\n",
    "A vital first step in the analysis of lung cancer CT scans is the detection of pulmonary nodules, which may or may not represent early stage lung cancer. Many Computer-Aided Detection (CAD) systems have already been proposed for this task. \n",
    "\n",
    "The LIDC/IDRI data set is publicly available, including the ground-truth annotations of nodules performed by four radiologists.\n",
    "\n",
    "This practical session is inspired from the challenge [LUNA16](https://luna16.grand-challenge.org/home/), which focused on a large-scale evaluation of automatic nodule detection algorithms on the LIDC/IDRI data set.\n",
    "\n",
    "<img src=\"images/lung_cancer1.png\">\n",
    "\n",
    "\n",
    "### Using LIDC/IDRI data set in this practical session\n",
    "\n",
    "For the needs of this practical session, we will look for an algorithm that only determines the likelihood for a given location in a CT scan to contain a pulmonary nodule. Furthermore, we have included a very small part of the LIDC/IDRI data set and we will use only a slice of suspicious regions of a CT scan.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "Originally our dataset was very unbalanced. There were a lot samples of non-nodules (negatives) but very few samples of nodules (positives). Machine learning algorithms and specifically neural networks and convolutional neural networks require to be trained on balanced dataset, meaning all the classes should be equally represented in the training set. \n",
    "\n",
    "Using data augmentation methods (**rotation and translation**), positive samples were massively augmented in order to balance the two classes (nodules and non-nodules)\n",
    "\n",
    "### Ground Truth data\n",
    "\n",
    "Categorical data are variables that contain label values rather than numeric values. In our dataset, a sample can represent a nodule or a non-nodule area, so initially we have our ground truth data in a categorical form. Many machine learning algorithms cannot operate on label data directly. They require all input variables to be numeric. This means that categorical data must be converted to a numerical form. This involves two steps:\n",
    "1. Integer Encoding\n",
    "2. One-Hot Encoding.\n",
    "\n",
    "As a first step, each unique category value is assigned an integer value. That's why in our dataset, a sample which represents a nodule will have as a label **1** and a sample which represents a non-nodule area will have as a label **0**.\n",
    "\n",
    "For our case, this enconding step would be enough since we have only two categories. However, in a more general problem with more than two classes, using this encoding allows the model to assume a natural ordering between categories which may result in poor performance or unexpected results. That's why, one hot encoding can be appled to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value. \n",
    "\n",
    "Therefore, a  nodule will be represented by the binary variable [1,0] and a non-nodule area by the binary variable [0,1].\n",
    "\n",
    "<img src=\"images/lung_cancer2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "77b5ce922c11056fda2cc0a605361876",
     "grade": false,
     "grade_id": "cell-88edf7d56c17540b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Visualize, load and prepare data for Machine Learning Algorithms\n",
    "\n",
    "CT images are stored in MetaImage (mhd/raw) format. Each .mhd file is stored with as a header file (.mhd) and a corresponding pixeldata file (.raw). To load a CT image, just **load the .mhd header file** and data from the binary .raw file will be automatically loaded.\n",
    "\n",
    "If you look carefully at the name of each .mhd file, you can extract useful information for a specific sample. You can identify its **number id**, its **size**, if the image was produced by a **data augmentation** method and if it contains a **nodule or not**.\n",
    "\n",
    "For example: *20046_x0y0z0_20x20x6_r0_1.mhd*\n",
    "* **20046**:   number of candidate patch\n",
    "* **x0y0z0**: no translation in any axis (if augmentation is used it is mentioned by the angle in the corresponding axis)\n",
    "* **20x20x6**: size of the image in voxels\n",
    "* **r0**:     no rotation\n",
    "* **1**:      it is a positive sample - represents a nodule\n",
    "* **.mhd**:   it is a mhd file (this is the file which can be loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eaa451a946fc33d891799f2910eb06af",
     "grade": false,
     "grade_id": "cell-6e65384d14dac861",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 1: Import necessary libraries\n",
    "\n",
    "Load all necessary libraries using the **import** statement and check for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 1: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "945c2ee4613c085addad9e466a2e2d03",
     "grade": false,
     "grade_id": "cell-1f6fe6d45fa0e86d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2: Inspect the dataset\n",
    "\n",
    "1. Using SimpleITK, create a function to load .mhd files (__Tip:__ The function should return a numpy array - sitk.GetArrayFromImage( ) )\n",
    "2. Visualize the 6 slices of a chosen patch using matplotlib subplot figure and mention if it is a positive or a negative patch.\n",
    "3. Visualize some augmented samples from the same candidate region using matplotlib subplot of the same patch and mention the augmentation method. Check visually if the observed patch was modified using the same augmentation method that is mentioned in the samples' filename.\n",
    "4. Count your files. How many positives and negatives there are in this dataset? (__Tip:__ Use glob library to get the number of specific files in your dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 1: Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dcf46ab947dd55a873e1469c775a2f59",
     "grade": false,
     "grade_id": "cell-d9e7fbe181dfe944",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 3: Data pre-processing\n",
    "In order to use neural-networks, we need to pre-process the data and store it in a way which can be easily interpreted by tensorflow.\n",
    "\n",
    "First of all, separate the test set from the train set. Keep 10% of your samples as a test-set. (**Tip:** Use sklearn **train_test_split** function.)\n",
    "\n",
    "Since 3D CNNs need a lot of computation power, we will convert our problem to 2D. Instead of using all the 6 CT slices, we will use only the 3rd CT slice. Therefore, from each sample, the 3rd slice wiil be extracted and further preprocessed. In addition, our values currently range from -1024 HU to around 2000 HU. Anything above 400 is not interesting to us, as these are simply bones with different radiodensity. A commonly used set of thresholds to normalize between is -1000 and 400. Finally the labels will be properly encoded to be used for training and testing.\n",
    "\n",
    "Write your pre-processing tasks as python functions (listed below) and in the end create a pipeline for each sample which will be also implemented as a function.\n",
    "1. Function loading the .mhd image as a numpy array.\n",
    "2. Function extracting the 3rd slice of a patch. The final form of patches should be a numpy array of size **20x20** pixels.\n",
    "3. Function normalizing the dataset.\n",
    "The unit of measurement in CT scans is the **Hounsfield Unit (HU)**, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this. From Wikipedia:\n",
    "<img src=\"images/HU_CTscannersCalibration.png\">\n",
    "\n",
    "    Create a function which is going to normalize the samples according to this table \n",
    "        * Create numpy arrays \n",
    "        * Normalize between [-1000, 400] using this normalization method \n",
    "    $npzarray = (npzarray - minHU) / (maxHU - minHU)$ <br>\n",
    "         where minHU = -1000 and maxHU = 400\n",
    "        * After this normalization set any values bigger than 1 to 1 and any values smaller than 0 to 0. \n",
    "\n",
    "4. If statement creating **1-hot** labels as a ground truth data in order to train the neural networks. (One-hot labels: [0,1] --> negative, [1,0]--> positive)\n",
    " \n",
    "5. Store into .npy data binary files. (__Tip:__ Use np.save to store data.)\n",
    "\n",
    "\n",
    "The pipeline should execute the functions as follows: <br>\n",
    "Load mhd using sitk -> normalize -> Extract 3rd slice -> Label \n",
    "\n",
    "and return:\n",
    "* data:  [sample_idx,20,20]\n",
    "* labels: [sample_idx,2]\n",
    "\n",
    "Data pre-processing should be done for both: train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 1: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f0f3e11c93d2004ee46bad5992f6c714",
     "grade": false,
     "grade_id": "cell-f4edb7482a0753c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In case you are not able to complete part 1, you can move to part 2 of the session by loading the provided .npy files. In that case you will **not** get any credits for Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "687da3742ff425ca860c8478ed7d081a",
     "grade": false,
     "grade_id": "cell-d1fb33251b563bb8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Model training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d1de1e961e4781cd28635929dc63f5d",
     "grade": false,
     "grade_id": "cell-a11370ef4ea445ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Using Keras - A High Level API for Tensorflow\n",
    "\n",
    "Keras introduces a High-Level API that makes neural network building and training fast and easy. This API is intuitive and fully compatible with Tensorflow. In addition, keras provides a plethora of [APIs](https://keras.io/api/models/) and the user may select one that meets his needs. For the purpose of this exercise, the [sequential model](https://keras.io/api/models/sequential/) can be employed. \n",
    "\n",
    "When you install TensorFlow and Keras, you automatically also install <b>Tensorboard</b>. During or after training you can use  it  to visualize the network and its performance. <b> For  all the following training tasks you are requested to include the loss and the accuracy graphs for training and also for evaluating the test-set</b>. A simple way to attach these graphs to jupyter notebook is to use Window's Snipping Tool or a similar application. Then display it either using a plain HTML ```<img src>``` or by using ```IPython.display.Image``` and ```IPython.core.display.HTML```. Don't forget to submit the actual image file with your .ipynb as the image is not saved in the notebook, it is just linked. Newer versions tensorboard allow to automatically include these figures in the ipython cells.\n",
    "\n",
    "### Some Tips:\n",
    "\n",
    "* Use ```np.load``` to load your data from the previously created .npy binary files.\n",
    "* Use ```reshape``` method when it is needed to import data to the network.\n",
    "* Make your own function to create the model for each network architecture to avoid any model conflicts.\n",
    "\n",
    "* If you trained a model in a past python session, you can load the trained model but first you have to <b>define and initialize it again</b>. \n",
    "\n",
    "* Use <b>Tensorbaord</b> to visualize network and performance:\n",
    "Open the command prompt or the anaconda prompt (or a terminal for linux/iOS) and in the working directory type:\n",
    "``` $ tensorboard --logdir='...' ```\n",
    "* You may need to add tensorboard's script to your SYSTEM'S PATH to be able to launch it. \n",
    "\n",
    "* <b>If you are using the tensorflow 2 it is possible to run tensorboad using the jupyter notebook. </b>\n",
    "In a jupyter cell type: ``` %tensorboard --logdir logs ```.\n",
    "For more information look [here](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)\n",
    "\n",
    "* If you have strange keras/tflearn errors, restart the python kernel and <b> re-run only the cells concerning your task you currently work (i.e. run the cells concerning only the current tfmodel). Sometimes you cannot load two different tfmodels at the same python session.</b>.\n",
    "\n",
    "(*) Depending the version of tflearn/tensorflow you are working some commands might differ. Please look online for recent documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8d6d8b691a25bac726e81fb9f5593de9",
     "grade": false,
     "grade_id": "cell-e3701a8ed58211ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Some Tensorboard examples.\n",
    "Here are some examples of a training session. After each task, expand accuracy and loss graphs and attach them in the next cell of the notebook like we did in this exmple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d3fdb9f0dbaf340f3a3cb972204745a",
     "grade": false,
     "grade_id": "cell-3399b1ef562b5cc9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<img src=\"screenshots_tensorboard/all_graphs.png\">\n",
    "<img src=\"screenshots_tensorboard/accuracy_graph.png\">\n",
    "<img src=\"screenshots_tensorboard/loss_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "839c62166cf59b1aba0ebb12f7e38d6d",
     "grade": false,
     "grade_id": "cell-d5025d3539123bdb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 1: Load Data from the numpy binary files (from Part 1)\n",
    "Use the numpy load function to load the data created in the previous part of the exercise. The data has to be reshaped in a specific way (see code below) in order to be compatible with the tflearn neural network input standard. After the data is successfully loaded, plot any patch and its ground truth as image title to verify that data is correctly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 1\n",
    "\n",
    "X = train_data.reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X_val = test_data.reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d1106dc6ff56598fb4c8afd41d9061f",
     "grade": false,
     "grade_id": "cell-0afd5980978d0695",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2: Artificial Neural Networks (1 layer)\n",
    "First, lets train our classifier using an artificial neural network with just one layer, a fully connected one. This layer will have *softmax* as activation function. \n",
    "\n",
    "The main advantage of using this activation function is the output range. The range of the outputs will be [0,1] and the sum of all the probabilities will be equal to one. Generally, if the softmax function is used for a  multi-classiication model it returns the probabilities of each class and the target class will have the high probability.\n",
    "\n",
    "Using TFLearn, define your model. Use these hyperparameters:\n",
    "* Learning Rate = 1e-4\n",
    "* Batch Size = 8\n",
    "\n",
    "Since the network is small, it is not recommended to use Dropout. Train first for 1-2 epochs to verify that everything works and then train for ~10 epochs. Observe the training and evaluate graphs from Tensorboard to decide when to stop training. Copy these graphs to your notebook and explain what happens.\n",
    "\n",
    "To evaluate the trained model, test it with the test samples and use various performance measures, like <b>confusion matrix</b> for different decision thresholds, <b>precision/recall</b> versus the decision threshold graph and finally plot a **ROC** curve. \n",
    "\n",
    "Explain these graphs and the ROC curve. What role does the decision threshold play and why it is so important? Can we have a single value as the decision threshold? Why?\n",
    "\n",
    "### Tip:\n",
    "* It is highly recommended to write all the tasks in functions and have as an input argument your tfmodel. Then you will be able to re-use them in the next tasks for different tfmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "729112bb739cb27832f2c27f1d3d3c5a",
     "grade": false,
     "grade_id": "cell-ec3cbbcde6634b2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 3: Test the model\n",
    "Use **predict** method to test a single test sample. Plot the patch and include in the title its ground truth and the prediction. Now, test all the test samples using a for loop and print the accuracy for a decision threshold of 0.5. Additionally, print the **confusion matrix** for the same decision threshold and comment on your findings. Do you think it's necessary to investigate our system's performance by setting different decision thresholds? What performance measurement should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2d112b399d5f76d26eb16384424a219",
     "grade": false,
     "grade_id": "cell-bfdd83f2ba53424c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Print the ROC curve and comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 3 - ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d82ad9587727760dd790c5846e9ba05f",
     "grade": false,
     "grade_id": "cell-ec462a6ad2168c48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 4.1: Artificial Neural Networks (3 layers) - Let's go deep!\n",
    "\n",
    "From now on, we will try various networks. Make sure with every network/parameter change, you update any hyperameters; you test and evaluate the new model with the previously used evaluation metrics (ROC curve, confusion matrix...). Moreover, every time include the tensorboard graphs which describe any new training session (training and validation, losses and accuracy).\n",
    "\n",
    "Let's try 3 fully-connected layers using **sigmoid** activation function. Choose different values for the learning rate, number of epochs and batch size (e.g. 10, 20, 30...). \n",
    "\n",
    "Here is an example network but you are free to choose your own network.\n",
    "* 1st FC with 80 neurons, sigmoid/relu\n",
    "* 2nd FC with 40 neurons, sigmoid/relu\n",
    "* 3rd FC with 2 neurons and softmax\n",
    "* optimizer: sgd or adam or ...\n",
    "\n",
    "Explain your network's architecture and **test** your system as you did for the previous simple neural network. What do you observe? Make sure you tune again any hyperparameters like the amount of training epochs. Print some graphs which indicate **underfitting** and **overfitting**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "822f2a649d5f2f9cebf5cfb29e5681f1",
     "grade": false,
     "grade_id": "cell-2f4b91a56afdcffa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 4.2: Relu!\n",
    "RELU = Rectified Linear Unit\n",
    "\n",
    "Let's try the same network but using RELU instead of sigmoid now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9daeec5dcb3d381740aeac3613a7b8ba",
     "grade": false,
     "grade_id": "cell-a6394cf303cb2289",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 4.3: Dropout!\n",
    "Use dropout. Some recommended values to investigate are 0.9, 0.5, 0.1. Explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
     "grade": false,
     "grade_id": "cell-2df072bbe1906a59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 5. A Convolutional Neural Network\n",
    "\n",
    "#### Model definition\n",
    "\n",
    "input -> conv1 -> pool1 -> conv2 -> fc1 -> fc2 -> softmax\n",
    "* convolution layers: kernel size = 3x3, number of channels = 64, activation function = sigmoid\n",
    "* max-pooling layers: kernel size = 2x2, strides = 2\n",
    "* 1st fully connected: 50 neurons, activation function = sigmoid\n",
    "* 2nd fully connected: 2 neurons, activation function = softmax\n",
    "* optimizer = sgd / adam\n",
    "* use dropout (e.g. 0.5)\n",
    "\n",
    "Like before, measure system's prerfomance and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
     "grade": false,
     "grade_id": "cell-2df072bbe1906a59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 6. A Deeper Convolutional Neural Network\n",
    "\n",
    "#### Model definition\n",
    "\n",
    "input -> conv1 -> pool1 -> conv2 -> conv3 -> fc1 -> fc2 -> softmax\n",
    "* convolution layers: kernel size = 5x5, number of channels = 64, activation function = relu\n",
    "* max-pooling layers: kernel size = 2x2, strides = 2\n",
    "* 1st fully connected: 100 neurons, activation function = relu\n",
    "* 2nd fully connected: 2 neurons, activation function = softmax\n",
    "* optimizer = sgd / adam\n",
    "* use dropout (e.g. 0.5)\n",
    "\n",
    "Like before, measure system's prerfomance and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for Part 2: Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d0ec0155912f600620327325c499bac",
     "grade": false,
     "grade_id": "cell-72468f1cc2255060",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### (BONUS) Create your own convolution neural network\n",
    "\n",
    "Create your own network and try to increase model's accuracy. You can use different:\n",
    "* size of convolutional kernels\n",
    "* activation functions\n",
    "* optimizers ...\n",
    "***\n",
    "\n",
    "* **Extra task**: Select a different part of the dataset as test-set and repeat the training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for BONUS part"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
